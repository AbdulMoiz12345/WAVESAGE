"""
integrated_single_process.py
---------------------------
Compute Integrated Gradients (IG) explanations for EEG windows
classified as normal or abnormal.

This script:
------------
1. Loads a trained EEG classification model.
2. Applies Integrated Gradients to compute feature attributions for an EEG window.
3. Identifies abnormal time segments based on attribution thresholds.
4. Computes explainability metrics (Coverage, Precision, IoU).
5. Visualizes EEG signal and corresponding importance map.

Before running:
---------------
- Place your model and scaler in:
    models/
        eeg_classifier.h5
        scaler.pkl

- Place EEG .npy files in:
    data/abnormal_windows/

- Run the script:
    python integrated_gradients_eeg.py

Output:
-------
- Plots showing EEG signal and important time segments.
- Console printout of metrics (Coverage, Precision, IoU).
- Can be adapted for batch processing across multiple files.

"""

# ------------------------------------------------------------
# Imports
# ------------------------------------------------------------
import os
import re
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt


# ------------------------------------------------------------
# Step 1: Integrated Gradients Computation
# ------------------------------------------------------------
@tf.function
def compute_gradients(inputs, target_class_idx):
    """Compute gradients of the target class output with respect to inputs."""
    with tf.GradientTape() as tape:
        tape.watch(inputs)
        predictions = model(inputs)
        target_class_predictions = predictions[:, target_class_idx]
    return tape.gradient(target_class_predictions, inputs)


def integrated_gradients(inputs, baseline, target_class_idx, m_steps=50):
    """Compute Integrated Gradients for a given input and baseline."""
    inputs = tf.convert_to_tensor(inputs)
    baseline = tf.convert_to_tensor(baseline)
    scaled_inputs = [baseline + (float(i) / m_steps) * (inputs - baseline) for i in range(m_steps + 1)]

    avg_gradients = tf.zeros_like(inputs)
    for scaled_input in scaled_inputs:
        gradients = compute_gradients(tf.convert_to_tensor(scaled_input), target_class_idx)
        avg_gradients += gradients

    avg_gradients /= tf.cast(m_steps, dtype=avg_gradients.dtype)
    return (inputs - baseline) * avg_gradients


# ------------------------------------------------------------
# Step 2: Utility Functions
# ------------------------------------------------------------
def extract_abnormal_segment_from_filename(file_path):
    """Extract ground-truth abnormal segment (start, end) from filename."""
    try:
        parts = re.findall(r'_(\d+\.\d+)', os.path.basename(file_path))
        return float(parts[-2]), float(parts[-1])
    except Exception:
        raise ValueError(f"‚ùå Could not extract abnormal segment from filename: {file_path}")


def calculate_metrics(real_segment, identified_segments):
    """Compute Coverage, Precision, IoU, and overlap statistics."""
    real_start, real_end = real_segment
    real_duration = real_end - real_start
    correct_overlap, identified_duration = 0, 0

    for (start, end) in identified_segments:
        identified_duration += (end - start)
        overlap_start = max(real_start, start)
        overlap_end = min(real_end, end)
        if overlap_start < overlap_end:
            correct_overlap += (overlap_end - overlap_start)

    coverage = correct_overlap / real_duration if real_duration > 0 else 0
    precision = correct_overlap / identified_duration if identified_duration > 0 else 0
    union = real_duration + identified_duration - correct_overlap
    iou = correct_overlap / union if union > 0 else 0

    return coverage, precision, iou


def get_time_segments(time_in_seconds, attributions, threshold, merge_gap=0.1):
    """Group consecutive time points exceeding threshold into segments."""
    segments = []
    start = None

    for i, attr in enumerate(attributions):
        if attr > threshold:
            if start is None:
                start = time_in_seconds[i]
        elif start is not None:
            end = time_in_seconds[i - 1]
            if segments and (start - segments[-1][1] <= merge_gap):
                segments[-1] = (segments[-1][0], end)
            else:
                segments.append((start, end))
            start = None

    if start is not None:
        end = time_in_seconds[-1]
        if segments and (start - segments[-1][1] <= merge_gap):
            segments[-1] = (segments[-1][0], end)
        else:
            segments.append((start, end))
    return segments


def load_eeg_signal(file_path):
    """Load EEG signal from a .npy file."""
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"‚ùå File not found: {file_path}")
    signal = np.load(file_path)
    print(f"üìÇ Loaded {file_path} | Shape: {signal.shape}")
    return signal


def set_baseline(signal):
    """Return a zero baseline of the same shape as the input signal."""
    return np.zeros(signal.shape)


# ------------------------------------------------------------
# Step 3: Visualization
# ------------------------------------------------------------
def plot_integrated_gradients(signal, attributions, title, file_path, sampling_frequency=200, threshold=30):
    """Visualize EEG signal and Integrated Gradients importance."""
    time_in_seconds = np.arange(len(signal)) / sampling_frequency
    attributions = np.array(attributions).flatten()

    # Normalize attributions
    positive_attrs = attributions[attributions > 0]
    if len(positive_attrs) > 0:
        min_attr, max_attr = np.min(positive_attrs), np.max(positive_attrs)
        normalized = np.zeros_like(attributions)
        normalized[attributions > 0] = 1 + 99 * (attributions[attributions > 0] - min_attr) / (max_attr - min_attr)
    else:
        normalized = np.zeros_like(attributions)

    segments = get_time_segments(time_in_seconds, normalized, threshold)
    real_segment = extract_abnormal_segment_from_filename(file_path)
    coverage, precision, iou = calculate_metrics(real_segment, segments)

    print(f"\nüìä Metrics for {os.path.basename(file_path)}")
    print(f"‚Ä¢ Coverage:  {coverage:.2f}")
    print(f"‚Ä¢ Precision: {precision:.2f}")
    print(f"‚Ä¢ IoU:       {iou:.2f}")
    print(f"‚Ä¢ Predicted Segments: {segments}\n")

    plt.figure(figsize=(10, 5))
    plt.plot(time_in_seconds, signal, color='blue', label='EEG Signal')
    plt.plot(time_in_seconds, normalized, color='red', alpha=0.7, label='Importance (Normalized)')
    plt.scatter(
        time_in_seconds[normalized > threshold],
        normalized[normalized > threshold],
        color='lime', label=f'Importance > {threshold}'
    )
    plt.xlabel("Time (seconds)")
    plt.ylabel("Amplitude / Importance")
    plt.title(title)
    plt.legend()
    plt.tight_layout()
    plt.show()


# ------------------------------------------------------------
# Step 4: Example Usage
# ------------------------------------------------------------
if __name__ == "__main__":
    # Example EEG window path (replace with your local path)
    FILE_PATH = "data/abnormal_windows/example_abnormal_window_0_1.23_1.56.npy"

    # Load model
    MODEL_PATH = "models/eeg_classifier.h5"
    if not os.path.exists(MODEL_PATH):
        raise FileNotFoundError(f"‚ùå Model not found at {MODEL_PATH}")
    model = tf.keras.models.load_model(MODEL_PATH)

    # Load signal and prepare for model
    signal = load_eeg_signal(FILE_PATH)
    signal = signal.reshape(1, len(signal), 1)
    baseline = set_baseline(signal)

    # Compute Integrated Gradients
    target_class_idx = 0  # 0 = abnormal class
    attributions = integrated_gradients(signal, baseline, target_class_idx)

    # Plot and evaluate
    plot_integrated_gradients(
        signal[0],
        attributions[0],
        title="Integrated Gradients ‚Äì EEG Abnormality Explanation",
        file_path=FILE_PATH,
        sampling_frequency=200,
        threshold=30
    )
