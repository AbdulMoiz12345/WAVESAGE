"""
rise_Batch_process.py
-----------------------
Apply RISE (Randomized Input Sampling for Explanation) on multiple EEG
windows to identify abnormal time segments and compute explainability metrics.

This script:
------------
1. Loads a trained EEG classification model and scaler.
2. Applies RISE to each `.npy` EEG window in a directory.
3. Extracts hand-labeled ground truth abnormal segments from filenames.
4. Calculates Coverage, Precision, and IoU between predicted and true segments.
5. Saves the results for all windows into a single CSV file.

Before running:
---------------
- Place your model and scaler in:
    models/
        eeg_classifier.h5
        scaler.pkl

- Place your EEG `.npy` files in:
    data/abnormal_windows_278/

- Run this script:
    python rise_explainer_batch.py

Outputs:
--------
- CSV file containing metrics for each EEG window.
- Console summary of results.

Author:
-------
TUKL EEG XAI Project â€“ RISE Analysis
"""

# ------------------------------------------------------------
# Imports
# ------------------------------------------------------------
import numpy as np
import matplotlib.pyplot as plt
from keras.models import load_model
import joblib
import os
import pandas as pd


# ------------------------------------------------------------
# Step 1: Paths and setup
# ------------------------------------------------------------
MODEL_PATH = "models/eeg_classifier.h5"
SCALER_PATH = "models/scaler.pkl"
DATA_DIR = "data/abnormal_windows_278"        # directory containing .npy EEG windows
OUTPUT_CSV = "results/rise_explainer_278.csv"
os.makedirs("results", exist_ok=True)

# Load model and scaler
if not os.path.exists(MODEL_PATH):
    raise FileNotFoundError(f"Model not found at {MODEL_PATH}")
if not os.path.exists(SCALER_PATH):
    raise FileNotFoundError(f"Scaler not found at {SCALER_PATH}")
if not os.path.exists(DATA_DIR):
    raise FileNotFoundError(f"EEG directory not found at {DATA_DIR}")

print("âœ… Loading model and scaler...")
model = load_model(MODEL_PATH)
scaler = joblib.load(SCALER_PATH)


# ------------------------------------------------------------
# Step 2: RISE and Utility Functions
# ------------------------------------------------------------
def generate_masks(num_masks, length, p=0.5):
    """Generate random binary masks for RISE."""
    return np.random.binomial(1, p, size=(num_masks, length)).astype(np.float32)

def apply_masks(input_signal, masks):
    """Apply binary masks to a 1D EEG signal."""
    return np.expand_dims(masks, axis=2) * input_signal

def rise_1d(model, input_signal, num_masks=10000, p=0.7):
    """Compute RISE importance map for 1D EEG input."""
    T = input_signal.shape[0]
    masks = generate_masks(num_masks, T, p)
    masked_inputs = apply_masks(input_signal, masks)
    preds = model.predict(masked_inputs, verbose=0).flatten()
    importance = preds.dot(masks) / num_masks
    importance = (importance - importance.min()) / (importance.max() - importance.min() + 1e-8)
    return importance

def get_intervals(indices, fs=200, merge_gap_sec=0.05):
    """Convert indices to time intervals and merge nearby intervals."""
    from itertools import groupby
    from operator import itemgetter
    intervals = []
    for _, g in groupby(enumerate(indices), lambda ix: ix[0] - ix[1]):
        group = list(map(itemgetter(1), g))
        intervals.append((group[0], group[-1]))
    merged = []
    for start, end in intervals:
        if not merged:
            merged.append((start, end))
        else:
            prev_start, prev_end = merged[-1]
            if start - prev_end <= int(merge_gap_sec * fs):
                merged[-1] = (prev_start, end)
            else:
                merged.append((start, end))
    return [(s / fs, e / fs) for s, e in merged]

def extract_hand_labeled_segment(file_path):
    """Extract ground truth (start, end) times from filename."""
    fname = os.path.basename(file_path)
    start, end = map(float, fname.replace(".npy", "").split("_")[-2:])
    return start, end

def calculate_metrics(actual_start, actual_end, predicted_segments):
    """Compute coverage, precision, and IoU for predicted vs actual segments."""
    real_duration = actual_end - actual_start
    correct_overlap = 0
    identified_duration = 0
    for seg in predicted_segments:
        s, e = seg
        identified_duration += (e - s)
        overlap_start = max(actual_start, s)
        overlap_end = min(actual_end, e)
        if overlap_start < overlap_end:
            correct_overlap += (overlap_end - overlap_start)
    coverage = correct_overlap / real_duration if real_duration > 0 else 0
    precision = correct_overlap / identified_duration if identified_duration > 0 else 0
    union_duration = real_duration + identified_duration - correct_overlap
    iou = correct_overlap / union_duration if union_duration > 0 else 0
    return coverage, precision, iou


# ------------------------------------------------------------
# Step 3: Process all EEG windows
# ------------------------------------------------------------
results = []
file_list = [f for f in os.listdir(DATA_DIR) if f.endswith(".npy")]
file_list.sort()

print(f"\nðŸ§  Processing {len(file_list)} EEG windows...\n")

for i, fname in enumerate(file_list, 1):
    file_path = os.path.join(DATA_DIR, fname)
    print(f"[{i}/{len(file_list)}] Processing: {fname}")

    # Load EEG window
    eeg = np.load(file_path)
    if eeg.ndim == 1:
        eeg = eeg.reshape(-1, 1)
    flat = eeg.reshape(1, -1)
    scaled = scaler.transform(flat).reshape(-1, 1)

    # Apply RISE
    importance = rise_1d(model, scaled, num_masks=10000, p=0.7)
    threshold = 0.6
    idxs = np.where(importance >= threshold)[0]
    predicted_segments = get_intervals(idxs, fs=200, merge_gap_sec=0.05)

    # Compute metrics
    actual_start, actual_end = extract_hand_labeled_segment(file_path)
    coverage, precision, iou = calculate_metrics(actual_start, actual_end, predicted_segments)

    # Append result
    results.append({
        "File": fname,
        "Actual_Start": actual_start,
        "Actual_End": actual_end,
        "Predicted_Intervals": [(float(s), float(e)) for s, e in predicted_segments],
        "Coverage": round(coverage, 4),
        "Precision": round(precision, 4),
        "IoU": round(iou, 4)
    })

    print(f"  â€¢ Coverage: {coverage:.3f}, Precision: {precision:.3f}, IoU: {iou:.3f}")

# ------------------------------------------------------------
# Step 4: Save results to CSV
# ------------------------------------------------------------
df = pd.DataFrame(results)
df.to_csv(OUTPUT_CSV, index=False)
print(f"\nâœ… Results saved to: {OUTPUT_CSV}")

# ------------------------------------------------------------
# Step 5: Summary Statistics
# ------------------------------------------------------------
avg_cov = df["Coverage"].mean()
avg_prec = df["Precision"].mean()
avg_iou = df["IoU"].mean()

print("\nðŸ“Š Average Metrics Across All Files:")
print(f" â€¢ Average Coverage:  {avg_cov:.3f}")
print(f" â€¢ Average Precision: {avg_prec:.3f}")
print(f" â€¢ Average IoU:       {avg_iou:.3f}")
